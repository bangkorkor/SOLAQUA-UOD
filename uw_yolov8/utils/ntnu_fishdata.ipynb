{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8c4743",
   "metadata": {},
   "source": [
    "# NTNU fish dataset processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f340bd",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "\n",
    "Using the NTNU-setup\n",
    "\n",
    "- Place the .zip in the data folder\n",
    "- Then unzip it manually or running code below:\n",
    "    - Delete _MACOSX folder if this also were addded.\n",
    "    - Inside FishDataset, Delete data.yaml, Delete Compiled Online Data ...  .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb1394d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FishDatas etextracted to: /Users/henrik/kode/SP/uw_yolov8/data\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"../data/NTNU_fish.zip\"\n",
    "extract_dir = \"../data/\"\n",
    "\n",
    "# Make sure the target directory exists\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"✅ FishDatas etextracted to:\", os.path.abspath(extract_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada94eb",
   "metadata": {},
   "source": [
    "### Make config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b839b926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/henrik/kode/SP/uw_yolov8/data/FishDataset/fishdataset.yaml\n",
      "\n",
      "Preview:\n",
      "\n",
      "\n",
      "path: /Users/henrik/kode/SP/uw_yolov8/data/FishDataset\n",
      "train: /Users/henrik/kode/SP/uw_yolov8/data/FishDataset/train/images\n",
      "val:   /Users/henrik/kode/SP/uw_yolov8/data/FishDataset/val/images\n",
      "test:  /Users/henrik/kode/SP/uw_yolov8/data/FishDataset/test/images\n",
      "\n",
      "nc: 1\n",
      "names: ['fish']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Base paths\n",
    "NOT_ROOT = Path.cwd()\n",
    "ROOT = NOT_ROOT.parent\n",
    "\n",
    "DATA = ROOT / \"data\" / \"FishDataset\"\n",
    "\n",
    "IMAGES_TRAIN = DATA / \"train\" / \"images\"\n",
    "IMAGES_VAL   = DATA / \"val\"   / \"images\"\n",
    "IMAGES_TEST  = DATA / \"test\"  / \"images\"\n",
    "\n",
    "# Build YAML (classic YOLO: train/val/test point to images dirs)\n",
    "dataset_yaml = f\"\"\"\n",
    "path: {DATA.as_posix()}\n",
    "train: {IMAGES_TRAIN.as_posix()}\n",
    "val:   {IMAGES_VAL.as_posix()}\n",
    "test:  {IMAGES_TEST.as_posix()}\n",
    "\n",
    "nc: 1\n",
    "names: ['fish']\n",
    "\"\"\"\n",
    "\n",
    "# Write YAML next to the dataset\n",
    "yaml_path = DATA / \"fishdataset.yaml\"\n",
    "yaml_path.write_text(dataset_yaml.strip() + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote:\", yaml_path)\n",
    "print(\"\\nPreview:\\n\")\n",
    "print(dataset_yaml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee75ad6",
   "metadata": {},
   "source": [
    "## Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b4ae092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ DATASET SUMMARY ================\n",
      "\n",
      "Root: /Users/henrik/kode/SP/uw_yolov8/data/FishDataset\n",
      "Classes (1): ['fish']\n",
      "\n",
      "[TRAIN]\n",
      "  images dir: /Users/henrik/kode/SP/uw_yolov8/data/FishDataset/train/images\n",
      "  labels dir: /Users/henrik/kode/SP/uw_yolov8/data/FishDataset/train/labels\n",
      "  #images: 17,956\n",
      "  #labels: 17,956\n",
      "  #instances: 43,890\n",
      "  per-class: {'fish': 43890}\n",
      "\n",
      "[VAL]\n",
      "  images dir: /Users/henrik/kode/SP/uw_yolov8/data/FishDataset/val/images\n",
      "  labels dir: /Users/henrik/kode/SP/uw_yolov8/data/FishDataset/val/labels\n",
      "  #images: 5,130\n",
      "  #labels: 5,130\n",
      "  #instances: 12,103\n",
      "  per-class: {'fish': 12103}\n",
      "\n",
      "[TEST]\n",
      "  images dir: /Users/henrik/kode/SP/uw_yolov8/data/FishDataset/test/images\n",
      "  labels dir: /Users/henrik/kode/SP/uw_yolov8/data/FishDataset/test/labels\n",
      "  #images: 2,566\n",
      "  #labels: 2,566\n",
      "  #instances: 5,721\n",
      "  per-class: {'fish': 5721}\n",
      "\n",
      "--------------- TOTALS ---------------\n",
      "Total images:    25,652\n",
      "Total labels:    25,652\n",
      "Total instances: 61,714\n",
      "\n",
      "===============================================\n",
      "\n",
      "Saved JSON report to: /Users/henrik/kode/SP/uw_yolov8/data/FishDataset/dataset_stats.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "# -------- Settings --------\n",
    "ENABLE_IMAGE_SIZE_STATS = False  # set True to also compute min/mean/max sizes (slower)\n",
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "# Project root is one level up from utils/\n",
    "ROOT = SCRIPT_DIR.parent\n",
    "DATA = ROOT / \"data\" / \"FishDataset\"\n",
    "\n",
    "def load_class_names(data_dir: Path):\n",
    "    yaml_path = data_dir / \"data.yaml\"\n",
    "    if yaml_path.exists():\n",
    "        try:\n",
    "            cfg = yaml.safe_load(yaml_path.read_text(encoding=\"utf-8\"))\n",
    "            names = cfg.get(\"names\")\n",
    "            # names can be list or dict {id:name}\n",
    "            if isinstance(names, dict):\n",
    "                # convert to index-ordered list\n",
    "                max_idx = max(int(k) for k in names.keys())\n",
    "                arr = [\"\"] * (max_idx + 1)\n",
    "                for k, v in names.items():\n",
    "                    arr[int(k)] = str(v)\n",
    "                return arr\n",
    "            elif isinstance(names, list):\n",
    "                return [str(n) for n in names]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fallback (your current dataset has 1 class: fish)\n",
    "    return [\"fish\"]\n",
    "\n",
    "def list_images(images_dir: Path):\n",
    "    return sorted([p for p in images_dir.rglob(\"*\") if p.suffix.lower() in IMAGE_EXTS])\n",
    "\n",
    "def list_labels(labels_dir: Path):\n",
    "    return sorted([p for p in labels_dir.rglob(\"*.txt\")])\n",
    "\n",
    "def safe_float(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def scan_split(split_dir: Path, class_names):\n",
    "    images_dir = split_dir / \"images\"\n",
    "    labels_dir = split_dir / \"labels\"\n",
    "\n",
    "    imgs = list_images(images_dir) if images_dir.exists() else []\n",
    "    lbls = list_labels(labels_dir) if labels_dir.exists() else []\n",
    "\n",
    "    img_stems = {p.stem for p in imgs}\n",
    "    lbl_stems = {p.stem for p in lbls}\n",
    "\n",
    "    images_without_labels = sorted([p.as_posix() for p in imgs if p.stem not in lbl_stems])\n",
    "    labels_without_images = sorted([p.as_posix() for p in lbls if p.stem not in img_stems])\n",
    "\n",
    "    # Parse labels\n",
    "    instance_count = 0\n",
    "    class_counter = Counter()\n",
    "    invalid_lines = []\n",
    "\n",
    "    for lp in lbls:\n",
    "        with lp.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f, start=1):\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith(\"#\"):\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                if len(parts) < 5 or not parts[0].isdigit() or not all(safe_float(x) for x in parts[1:5]):\n",
    "                    invalid_lines.append(f\"{lp.as_posix()}:{i} -> {line}\")\n",
    "                    continue\n",
    "                cls_id = int(parts[0])\n",
    "                class_counter[cls_id] += 1\n",
    "                instance_count += 1\n",
    "\n",
    "    size_stats = {}\n",
    "    if ENABLE_IMAGE_SIZE_STATS:\n",
    "        try:\n",
    "            from PIL import Image\n",
    "            widths, heights = [], []\n",
    "            for p in imgs:\n",
    "                try:\n",
    "                    with Image.open(p) as im:\n",
    "                        w, h = im.size\n",
    "                        widths.append(w); heights.append(h)\n",
    "                except Exception:\n",
    "                    # Skip unreadable images\n",
    "                    pass\n",
    "            if widths and heights:\n",
    "                import statistics as stats\n",
    "                size_stats = {\n",
    "                    \"count_measured\": len(widths),\n",
    "                    \"width_min\": min(widths), \"width_max\": max(widths), \"width_mean\": round(stats.mean(widths), 2),\n",
    "                    \"height_min\": min(heights), \"height_max\": max(heights), \"height_mean\": round(stats.mean(heights), 2),\n",
    "                }\n",
    "        except ImportError:\n",
    "            size_stats = {\"note\": \"Pillow not installed; set ENABLE_IMAGE_SIZE_STATS=True after `pip install pillow`\"}\n",
    "\n",
    "    # Map class ids to names\n",
    "    id2name = {i: (class_names[i] if i < len(class_names) else f\"class_{i}\") for i in class_counter.keys()}\n",
    "    class_breakdown = {id2name[i]: class_counter[i] for i in sorted(class_counter.keys())}\n",
    "\n",
    "    return {\n",
    "        \"images_dir\": images_dir.as_posix(),\n",
    "        \"labels_dir\": labels_dir.as_posix(),\n",
    "        \"num_images\": len(imgs),\n",
    "        \"num_labels\": len(lbls),\n",
    "        \"images_without_labels\": images_without_labels,\n",
    "        \"labels_without_images\": labels_without_images,\n",
    "        \"instances_total\": instance_count,\n",
    "        \"instances_per_class\": class_breakdown,\n",
    "        \"invalid_label_lines\": invalid_lines[:50],  # preview first 50\n",
    "        \"invalid_label_lines_count\": len(invalid_lines),\n",
    "        \"image_size_stats\": size_stats,\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    if not DATA.exists():\n",
    "        raise SystemExit(f\"Dataset not found at: {DATA.as_posix()}\")\n",
    "\n",
    "    class_names = load_class_names(DATA)\n",
    "\n",
    "    report = {\"dataset_root\": DATA.as_posix(), \"classes\": class_names, \"splits\": {}}\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        split_dir = DATA / split\n",
    "        if split_dir.exists():\n",
    "            report[\"splits\"][split] = scan_split(split_dir, class_names)\n",
    "        else:\n",
    "            report[\"splits\"][split] = {\"note\": f\"{split} split not found\"}\n",
    "\n",
    "    # Print a friendly summary\n",
    "    print(\"\\n================ DATASET SUMMARY ================\\n\")\n",
    "    print(f\"Root: {report['dataset_root']}\")\n",
    "    print(f\"Classes ({len(class_names)}): {class_names}\\n\")\n",
    "\n",
    "    grand_images = grand_labels = grand_instances = 0\n",
    "    for split, info in report[\"splits\"].items():\n",
    "        if \"note\" in info:\n",
    "            print(f\"[{split.upper()}] {info['note']}\\n\")\n",
    "            continue\n",
    "        print(f\"[{split.upper()}]\")\n",
    "        print(f\"  images dir: {info['images_dir']}\")\n",
    "        print(f\"  labels dir: {info['labels_dir']}\")\n",
    "        print(f\"  #images: {info['num_images']:,}\")\n",
    "        print(f\"  #labels: {info['num_labels']:,}\")\n",
    "        print(f\"  #instances: {info['instances_total']:,}\")\n",
    "        if info[\"instances_per_class\"]:\n",
    "            print(f\"  per-class: {info['instances_per_class']}\")\n",
    "        if info[\"images_without_labels\"]:\n",
    "            print(f\"  images without labels: {len(info['images_without_labels'])} (showing up to 5)\")\n",
    "            for p in info[\"images_without_labels\"][:5]:\n",
    "                print(f\"    - {p}\")\n",
    "        if info[\"labels_without_images\"]:\n",
    "            print(f\"  labels without images: {len(info['labels_without_images'])} (showing up to 5)\")\n",
    "            for p in info[\"labels_without_images\"][:5]:\n",
    "                print(f\"    - {p}\")\n",
    "        if info[\"invalid_label_lines_count\"] > 0:\n",
    "            print(f\"  invalid label lines: {info['invalid_label_lines_count']} (showing up to 5)\")\n",
    "            for ln in info[\"invalid_label_lines\"][:5]:\n",
    "                print(f\"    - {ln}\")\n",
    "        if info[\"image_size_stats\"]:\n",
    "            print(f\"  image size stats: {info['image_size_stats']}\")\n",
    "        print()\n",
    "        grand_images += info.get(\"num_images\", 0)\n",
    "        grand_labels += info.get(\"num_labels\", 0)\n",
    "        grand_instances += info.get(\"instances_total\", 0)\n",
    "\n",
    "    print(\"--------------- TOTALS ---------------\")\n",
    "    print(f\"Total images:    {grand_images:,}\")\n",
    "    print(f\"Total labels:    {grand_labels:,}\")\n",
    "    print(f\"Total instances: {grand_instances:,}\")\n",
    "    print(\"\\n===============================================\\n\")\n",
    "\n",
    "    # Save a machine-readable copy\n",
    "    out_json = DATA / \"dataset_stats.json\"\n",
    "    out_json.write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"Saved JSON report to: {out_json.as_posix()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
